---
layout: default
title: My MSc
lang: en
ref: master-track
---
<h2>MSc track</h2>

Compiling my activities as a MSc candidate in the Computer Science Department from the Institute of Mathematics and Statistics from the University of Sao Paulo.

<ul>
  <li><a href="#grade">Program work</a> </li>
  <li><a href="#courses">Courses</a> </li>
  <li><a href="#books">Books</a> </li>
  <li><a href="#papers">Papers</a> </li>
  <li><a href="#misc">Misc</a> </li>
</ul>

<h3 id="grade">Program Work</h3>
<h3 id="courses">Courses</h3> 
<h3 id="books">Books</h3> 
<h3 id="papers">Papers</h3>
<ul>
  <li><a href="http://www.pnas.org/content/pnas/113/27/7310.full.pdf">Causal Inference in eocnomics and marking</a>
    <div></div>
  </li>

  <li><a href="https://arxiv.org/abs/1804.10846">Data science is science's second chance to get causal inference right: A classification of data science tasks</a>
    <div>A cool trial to categorize data science tasks in three groups: description, prediction and causal inference.
</br></br>
      Description is for quantitative summary: computing proportions, mean, clustering and other visualizations.
      Prediction is the mapping of some inputs (X) to output(s) (y). It can be simple like quantifying the association between two variables and complex when using hundreds of features to predict a probability for a phenomena occurrence. So it goes from calculation the correlation coefficient to building models like random forests, neural networks and so on.
</br></br>
      Causal Inference is "using data to calculate certain feature of the world if the world had been different (that is, causal inference is counterfactual prediction)". An example would be the estimation of the mortality rate that would have been observed if all individuals in a study population had received screening for colorectal cancer versus if they had not received screening.
</br></br>
    Most of successful applications today in Data Science are merely predictive tasks. It happens because "a successful prediction only requires three elements: 1) a large dataset with inputs and outputs; 2) an algorithm that establishes a mapping between inputs and outputs; and 3) a metric to assess the performance of the mapping, often based on a gold standard.". After having the task defined and gathering the data, all the information required is in the data. No domain-specific knowledge is needed. 
</br></br>
      For causal inference the expert knowledge is needed, usually in the form of unverifiable causal assumptions. That is, even after specifying a well-defined causal task and acquiring relevant data, subject-matter knowledge is necessary to guide the data analysis and to provide a justification for endowing the result numerical  estimates with a causal interpretation. There's an example for health records database about maternal smoking during pregnancy and the risk of infant mortality. 

<div align="center">
<figure>
	<a href="../../../images/master/prediction-causal-table.png">
		<img  style="width:450px;margin:10px" src="../../../images/master/prediction-causal.png"/>
	</a>
	<figcaption>Table from article</figcaption>
</figure>
</div>

      Randomly assigned treatment is a way of not relying on the expert knowledge to be able to estimate the average treatment effect. 
</br></br>
When the relevant expert knowledge can be easily encoded and incorporated into the algorithms the distinction between predictive and causal inference becomes unnecessary like in a Go playing system. It can easily computes the counterfactual of a play. 
</br></br>
      But most Auto "Most health and social scientists work with complex systems whose governing laws (the “rules of the game”) are not known in sufficient detail, it is unknown whether all necessary data are available, there is random error, and learning by trial and error is impossible."
</br></br>
    </div>
  </li>
</ul>

<h3 id="misc">Misc</h3>

<!-- <ul>  -->
<!-- 	<li>Python</li> -->
<!-- 	<li>Numpy</li> -->
<!-- 	<li>Pandas</li> -->
<!-- 	<li>Scikit-learn</li> -->
<!-- 	<li>Keras</li> -->
<!-- 	<li>Scipy stats</li> -->
<!-- 	<li>Matplotlib and Seaborn</li> -->
<!-- </ul> -->

<!-- <h3>Personal/College Projects: </h3> -->
<!-- <ul> -->
<!-- 	<li> -->
<!-- 		<p>  -->
<!-- 		E-mail classifier for Code Club Brasil with a multilabel approach, the idea is to build an automated email reply.     -->
<!-- 		The Notebook with my first experiments can be seen <a href="projects/ccbr emails - apr.html">here.</a></p> -->
<!-- 		<p>  -->
<!-- 		<a href="projects/example1.png">A bad prediction example.</a> -->
<!-- 		</p>	 -->
<!-- 		<p>  -->
<!-- 		<a href="projects/example2.png">A good prediction example.</a> -->
<!-- 		</p>	 -->
<!-- 	</li> -->
<!-- 	<li> -->
<!-- 	  <p> -->
<!-- 	    Tio-Pytinhas: a classifier for single coins and a regressor for group of coins. <p> <a href="https://github.com/lgmoneda/tio-pytinhas">Github</a></p><p><a href="https://www.youtube.com/watch?v=0ttcpYFPB58">Early stage video </a></p> -->
<!-- 	    </p> -->
<!-- 	</li> -->

<!-- 	<li> -->
<!-- 	  <p> -->
<!-- 	    Wally-IPS: A simple web panel to be feed with data from an IPS (Indoor Positioning System) application and generate some visualizations. <p> <a href="http://lgmoneda.github.io/projects/">More in projects page</a></p> -->
<!-- 	    </p> -->
<!-- 	</li> -->

<!-- 	<li> -->
<!-- 	  <p> -->
<!-- 	    Reinforcement Learning with Yellow Submarine: followed the open-ai interface to build an environment and agent to follow a path.  <p> <a href="https://github.com/lgmoneda/yellowsub">Github</a></p> <p><a href="https://www.youtube.com/watch?v=4CmgF1xS_5s">Video example </a> -->
<!-- 	    </p> -->
<!-- 	</li> -->
<!-- </ul> -->

<!-- <h3> Hackatons / Competitions </h3> -->
<!-- <p> -->
<!-- 	My <a href="https://www.kaggle.com/lgmoneda">Kaggle Profile</a>.  -->
<!-- </p> -->
<!-- <ul> -->
<!-- 	<li> -->
<!-- 		<p> -->
<!-- 		 	<a href="http://www.datasciencegame.com/ ">DataScienceGame 2016</a>'s selection: An image classification competition between teams from universities runned in <a href="https://inclass.kaggle.com/c/data-science-game-2016-online-selection">Kaggle</a>. Result: 38<span style="vertical-align: super; font-size: 0.5em;">th</span> in 117 teams. Though we haven't classified to the second phase, it was a great learning oportunity. Some code used in it:    -->
<!-- 		</p> -->
<!-- 		<p>  -->
<!-- 		<a href="hackatons/3/data_processing.html">Adjusting the data.</a> -->
<!-- 		</p>	 -->
<!-- 		<p>  -->
<!-- 		<a href="hackatons/3/modelo1.html">Simple ConvNet.</a> -->
<!-- 		</p> -->
<!-- 		<p>  -->
<!-- 		<a href="hackatons/3/learning_transfer.html">Learning transfer with VGG16.</a> -->
<!-- 		</p> -->
<!-- 	</li> -->
<!-- 	<li> -->
<!-- 		<p> -->
<!-- 		 	Kaggle's <a href="https://www.kaggle.com/c/sf-crime">San Francisco Crime Classification</a>: A classic multi-class problem. My first competition in Kaggle. Result: 198<span style="vertical-align: super; font-size: 0.5em;">th</span> in 2335 teams. Finished almost 2 months ago, but no private leaderboard yet. Here one notebook i used <a href="hackatons/2/the crimes.html">notebook</a>, though some grid search and xgboost were done in different files.    -->
<!-- 		</p> -->
<!-- 	</li> -->
<!-- 	<li> -->
<!-- 		<p> -->
<!-- 		 	My very first hackaton, i wasn't using Pandas neither Jupyter Notebook, so a lot of things there i do now with a few lines with Pandas! It was more of a training hackaton that resets over the time. Result: 6-10<span style="vertical-align: super; font-size: 0.5em;">th</span> in 120 competitors. Here's the <a href="https://github.com/lgmoneda/hackatons/blob/master/avidhya/loan-prediction/loan-hackaton.py">code</a>.    -->
<!-- 		</p> -->
<!-- 	</li> -->
<!-- </ul> -->

<!-- <h3>Introduction to Computer Intelligence Assignments (Poli 2016/1)</h3> -->
<!-- <p>  -->
<!-- 	Some basic ML Models built from scratch, i tried to follow scikit-learn interface. -->
<!-- </p> -->
<!-- <img style="float:right;width:300px;" src="PTC2669/xor2.png" alt="24/07/2016"> -->
<!-- <ul> -->
<!-- 	<li> -->
<!-- 		<p> -->
<!-- 			Linear Regression using Least Mean Squares, plots from weights evolution, learning curves and fitted model: <a href="PTC2669/PTC2669 - Lista 1 LMS Final.html">notebook.</a> -->
<!-- 		</p> -->
<!-- 	</li> -->
<!-- 	<li> -->
<!-- 		<p> -->
<!-- 			The Perceptron, plots with classes and the learned boundary line: <a href="PTC2669/Lista 3 - PTC2669.html">notebook.</a> -->
<!-- 		</p> -->
<!-- 	</li> -->
<!-- 	<li> -->
<!-- 		<p> -->
<!-- 			Multi Layer Perceptron solving the XOR problem: <a href="PTC2669/Lista 4 - PTC2669.html">notebook.</a> -->
<!-- 		</p> -->
<!-- 	</li> -->
<!-- </ul> -->

<!-- <h3>Machine Learning Specialization - Washington University (Coursera)</h3> -->
<!-- <p> Nice series of courses. We use numpy in the low level assignments (implementing algorithms) and graphlab, a ml library from Turi (former Dato), in the high level ones. They give us a skeleton, so the code isn't entirely written by me. I'm doing the course 4 now.	 -->
<!-- </p> -->
<!-- <ol> -->
<!-- 	<li> -->
<!-- 		<h4>A case study approach </h4> -->
<!-- 		<ol> -->
<!-- 			<li><a href="UW/1/Predicting+house+prices">Predicting House Prices.</a></li> -->
<!-- 			<li><a href="UW/1/Analyzing+product+sentiment">Analyzing product sentiment.</a></li> -->
<!-- 			<li><a href="UW/1/Analyzing+product+sentiment2">Analyzing product sentiment II.</a></li> -->
<!-- 			<li><a href="UW/1/Document+retrieval.html">Document retrieval.</a></li> -->
<!-- 			<li><a href="UW/1/Song+recommender">Song recommender.</a></li> -->
<!-- 			<li><a href="UW/1/Deep+Features+for+Image+Classification">Deep Features for Image Classification.</a></li> -->
<!-- 			<li><a href="UW/1/Deep+Features+for+Image+Retrieval.html">Deep Features for Image Retrieval.</a></li> -->
<!-- 		</ol> -->
<!-- 	</li> -->
<!-- 	<li> -->
<!-- 	<h4>Regression</h4> -->
<!-- 	<ol> -->
<!-- 		<li><a href="UW/2/week-1-simple-regression-done">Simple Regression.</a></li> -->
<!-- 		<li><a href="UW/2/week-2-multiple-regression-assignment-1-blank">Multiple Regression.</a></li> -->
<!-- 		<li><a href="UW/2/week-2-multiple-regression-assignment-2-blank">Multiple Regression II (Gradient descent).</a></li> -->
<!-- 		<li><a href="UW/2/week-3-polynomial-regression-assignment-blank">Polynomial Regression.</a></li> -->
<!-- 		<li><a href="UW/2/week-4-ridge-regression-assignment-1-blank">Ridge Regression.</a></li> -->
<!-- 		<li><a href="UW/2/week-4-ridge-regression-assignment-2-blank">Ridge Regression II.</a></li> -->
<!-- 		<li><a href="UW/2/week-5-lasso-assignment-1-blank.html">Lasso Regression.</a></li> -->
<!-- 		<li><a href="UW/2/week-5-lasso-assignment-2-blank.html">Lasso Regression II.</a></li> -->
<!-- 		<li><a href="UW/2/week-6-local-regression-assignment-blank.html">Local Regression.</a></li> -->
<!-- 	</ol> -->
<!-- 	</li> -->
<!-- 		<li> -->
<!-- 	<h4>Classification</h4> -->
<!-- 		<ol> -->
<!-- 		<li><a href="UW/3/module-2-linear-classifier-assignment-blank">Linear classifier.</a></li> -->
<!-- 		<li><a href="UW/3/module-3-linear-classifier-learning-assignment-blank">Logistic Regression.</a></li> -->
<!-- 		<li><a href="UW/3/module-4-linear-classifier-regularization-assignment-blank">Logistic Regression with L2 regularization.</a></li> -->
<!-- 		<li><a href="UW/3/module-5-decision-tree-assignment-1-blank">Decision Tree.</a></li> -->
<!-- 		<li><a href="UW/3/module-5-decision-tree-assignment-2-blank">Binary Decision Tree.</a></li> -->
<!-- 		<li><a href="UW/3/module-6-decision-tree-practical-assignment-blank">Decision Tree in Pratice.</a></li> -->
<!-- 		<li><a href="UW/3/module-8-boosting-assignment-1-blank.html">Exploring Ensemble methods..</a></li> -->
<!-- 		<li><a href="UW/3/module-8-boosting-assignment-2-blank">Boosting a Decision stump.</a></li> -->
<!-- 		<li><a href="UW/3/module-9-precision-recall-assignment-blank.html">Exploring Precision and Recall.</a></li> -->
<!-- 		<li><a href="UW/3/module-10-online-learning-assignment-blank.html">Stochastic Gradient Descent.</a></li> -->
<!-- 	</ol> -->
<!-- 	</li> -->
<!-- </ol> -->

<!-- <h3>Data Analysis and Interpretation Specialization (Coursera, 2016/1)</h3> -->
<!-- <p>  -->
<!-- 	Only the two first courses from the specialization. It was a good oportunity to review hypothesis tests and uso scipy.stats. The assignments are <a href="http://lgmoneda.github.io/data_science_coursera/">here</a>. -->
<!-- </p> -->

<!-- <h3>Others</h3> -->
<!-- <p>  -->
<!-- 	There're other courses that are related with Machine Learning, Data Science and Python, they're listed in my <a href="http://lgmoneda.github.io/cv/">cv</a>. -->
<!-- </p> -->
